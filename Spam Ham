#!/usr/bin/env python
# coding: utf-8

# In[1]:


import nltk


# In[ ]:


nltk.download_shell()


# In[3]:


messages = [line.rstrip() for line in open('SMSSpamCollection')]


# In[4]:


messages[0]


# In[6]:


import pandas as pd


# In[7]:


import seaborn as sns
sns.set_style('whitegrid')


# In[8]:


messages = pd.read_csv('SMSSpamCollection',sep='\t',names=['label','message'])


# In[9]:


messages.head()


# In[10]:


messages['length'] = messages['message'].apply(len)


# In[11]:


messages.head()


# In[12]:


import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')


# In[13]:


messages['length'].plot.hist(bins=100)


# In[15]:


messages.hist(column='length',by='label',bins=50,figsize=(14,6))


# In[17]:


import string


# In[18]:


from nltk.corpus import stopwords


# In[ ]:





# In[20]:


mess = 'Sample message! Notice: it has punctuation.'


# In[21]:


nopunc = [c for c in mess if c not in string.punctuation]


# In[24]:


nopunc = ''.join(nopunc)


# In[25]:


nopunc


# In[26]:


nopunc.split()


# In[27]:


clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]


# In[28]:


clean_mess


# In[35]:


def text_process(messages):
    
    '''
    1. Remove punctuation
    2. Remove stopwords
    3. Return list of clean words
    '''
    nopunc= [char for char in messages if char not in string.punctuation]
    
    nopunc = ''.join(nopunc)
    
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]


# In[36]:


messages['message'].head(5).apply(text_process)


# In[37]:


from sklearn.feature_extraction.text import CountVectorizer


# In[63]:


bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])


# In[66]:


print(len(bow_transformer.vocabulary_))


# In[67]:


mess4 = messages['message'][3]


# In[68]:


mess4


# In[70]:


bow4 = bow_transformer.transform([mess4])


# In[71]:


print(bow4)


# In[45]:


print(bgw4.shape)


# In[72]:


messages_bow = bow_transformer.transform(messages['message'])


# In[73]:


print(messages_bow.shape)


# In[74]:


from sklearn.feature_extraction.text import TfidfTransformer


# In[94]:


tfid_transformer = TfidfTransformer().fit(messages_bow)


# In[95]:


tfid4 = tfid.transform(bow4)


# In[96]:


print(tfid4)


# In[100]:


tfid_transformer.idf_[bow_transformer.vocabulary_['university']]


# In[86]:


message_tfid = tfid.transform(messages_bow)


# In[87]:


from sklearn.naive_bayes import MultinomialNB


# In[88]:


spam_detector = MultinomialNB().fit(message_tfid,messages['label'])


# In[89]:


spam_detector.predict(tfid4)[0]


# In[90]:


all_pred = spam_detector.predict(message_tfid)


# In[91]:


all_pred


# In[101]:


from sklearn.cross_validation import train_test_split


# In[102]:


msg_train,msg_test,label_train,label_test = train_test_split(messages['message'],messages['label'],test_size=0.3)


# In[92]:


from sklearn.pipeline import Pipeline


# In[103]:


pipeline = Pipeline([
    ('bow',CountVectorizer(analyzer=text_process)),
    ('tfid',TfidfTransformer()),
    ('classifier',MultinomialNB())
])


# In[104]:


pipeline.fit(msg_train,label_train)


# In[107]:


predictions = pipeline.predict(msg_test)


# In[106]:


from sklearn.metrics import classification_report


# In[109]:


print(classification_report(label_test,predictions))




